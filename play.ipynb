{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: torch in d:\\software\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in d:\\software\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Collecting transformers\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/20/37/1f29af63e9c30156a3ed6ebc2754077016577c094f31de7b2631e5d379eb/transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Collecting datasets\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/4c/37/22ef7675bef4ffe9577b937ddca2e22791534cbbe11c30714972a91532dc/datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Requirement already satisfied: tiktoken in d:\\software\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Collecting wandb\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/ce/be/020d511f537d3809ffb3fcedb855f5bf1daaf96ce2e0248022d015663332/wandb-0.19.7-py3-none-win_amd64.whl (20.3 MB)\n",
      "Requirement already satisfied: tqdm in d:\\software\\anaconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: filelock in d:\\software\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\software\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in d:\\software\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\software\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\software\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in d:\\software\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/ae/05/75b90de9093de0aadafc868bb2fa7c57651fd8f45384adf39bd77f63980d/huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\software\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\software\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\software\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\software\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/44/69/d21eb253fa91622da25585d362a874fa4710be600f0ea9446d8d0217cec1/tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/69/e2/b011c38e5394c4c18fb5500778a55ec43ad6106126e74723ffaee246f56e/safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/ff/77/e62aebd343238863f2c9f080ad2ef6ace25c919c6ab383436b5b81cbeef7/pyarrow-19.0.1-cp311-cp311-win_amd64.whl (25.3 MB)\n",
      "     ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.2/25.3 MB 4.1 MB/s eta 0:00:07\n",
      "      --------------------------------------- 0.4/25.3 MB 4.6 MB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.7/25.3 MB 4.9 MB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.9/25.3 MB 4.7 MB/s eta 0:00:06\n",
      "     - -------------------------------------- 1.1/25.3 MB 4.8 MB/s eta 0:00:06\n",
      "     - -------------------------------------- 1.2/25.3 MB 4.2 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 1.4/25.3 MB 4.6 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 1.6/25.3 MB 4.4 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.9/25.3 MB 4.5 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 2.1/25.3 MB 4.7 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 2.4/25.3 MB 4.6 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 2.7/25.3 MB 4.8 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 2.8/25.3 MB 4.7 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 3.0/25.3 MB 4.8 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 3.4/25.3 MB 4.9 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 3.4/25.3 MB 4.8 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 3.6/25.3 MB 4.6 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 3.7/25.3 MB 4.6 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 3.9/25.3 MB 4.4 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 4.1/25.3 MB 4.4 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 4.3/25.3 MB 4.4 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 4.5/25.3 MB 4.4 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 4.6/25.3 MB 4.3 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 4.8/25.3 MB 4.3 MB/s eta 0:00:05\n",
      "     ------- -------------------------------- 4.9/25.3 MB 4.3 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 5.1/25.3 MB 4.3 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 5.3/25.3 MB 4.3 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 5.4/25.3 MB 4.2 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 5.6/25.3 MB 4.2 MB/s eta 0:00:05\n",
      "     -------- ------------------------------- 5.7/25.3 MB 4.1 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 5.8/25.3 MB 4.0 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 5.9/25.3 MB 4.0 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 6.0/25.3 MB 4.0 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 6.2/25.3 MB 3.9 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 6.3/25.3 MB 3.9 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 6.4/25.3 MB 3.9 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 6.5/25.3 MB 3.8 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 6.6/25.3 MB 3.8 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 6.7/25.3 MB 3.7 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 6.7/25.3 MB 3.7 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 6.8/25.3 MB 3.7 MB/s eta 0:00:06\n",
      "     ---------- ----------------------------- 6.9/25.3 MB 3.6 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 7.0/25.3 MB 3.6 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 7.0/25.3 MB 3.5 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 7.1/25.3 MB 3.5 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 7.2/25.3 MB 3.5 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 7.4/25.3 MB 3.4 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 7.4/25.3 MB 3.4 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 7.5/25.3 MB 3.3 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 7.6/25.3 MB 3.3 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 7.7/25.3 MB 3.3 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 7.8/25.3 MB 3.3 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 7.9/25.3 MB 3.2 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 8.0/25.3 MB 3.2 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 8.0/25.3 MB 3.2 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 8.1/25.3 MB 3.2 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 8.2/25.3 MB 3.1 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 8.3/25.3 MB 3.1 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 8.4/25.3 MB 3.1 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 8.5/25.3 MB 3.1 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 8.6/25.3 MB 3.1 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 8.7/25.3 MB 3.1 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 8.8/25.3 MB 3.0 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 8.9/25.3 MB 3.0 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 9.0/25.3 MB 3.0 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 9.0/25.3 MB 3.0 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 9.1/25.3 MB 3.0 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 9.2/25.3 MB 2.9 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 9.2/25.3 MB 2.9 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 9.3/25.3 MB 2.9 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 9.4/25.3 MB 2.9 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 9.5/25.3 MB 2.9 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 9.6/25.3 MB 2.9 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 9.7/25.3 MB 2.8 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 9.7/25.3 MB 2.8 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 9.8/25.3 MB 2.8 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 10.0/25.3 MB 2.8 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 10.1/25.3 MB 2.8 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 10.2/25.3 MB 2.8 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 10.3/25.3 MB 2.8 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 10.4/25.3 MB 2.8 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 10.4/25.3 MB 2.8 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 10.5/25.3 MB 2.8 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 10.6/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 10.7/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 10.7/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 10.8/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 10.9/25.3 MB 2.6 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 10.9/25.3 MB 2.6 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 11.0/25.3 MB 2.6 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 11.1/25.3 MB 2.5 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 11.2/25.3 MB 2.5 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 11.3/25.3 MB 2.5 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 11.4/25.3 MB 2.5 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 11.5/25.3 MB 2.5 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 11.6/25.3 MB 2.5 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 11.6/25.3 MB 2.5 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 11.7/25.3 MB 2.5 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 11.8/25.3 MB 2.4 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 11.9/25.3 MB 2.4 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 11.9/25.3 MB 2.4 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 12.0/25.3 MB 2.4 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.1/25.3 MB 2.4 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.2/25.3 MB 2.4 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.3/25.3 MB 2.3 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.4/25.3 MB 2.3 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.5/25.3 MB 2.3 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.6/25.3 MB 2.3 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 12.7/25.3 MB 2.3 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 12.9/25.3 MB 2.3 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 13.1/25.3 MB 2.3 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 13.2/25.3 MB 2.3 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 13.3/25.3 MB 2.2 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 13.3/25.3 MB 2.2 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 13.4/25.3 MB 2.2 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 13.6/25.3 MB 2.2 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 13.6/25.3 MB 2.2 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 13.7/25.3 MB 2.2 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 13.8/25.3 MB 2.2 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 13.9/25.3 MB 2.2 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 14.0/25.3 MB 2.2 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 14.1/25.3 MB 2.2 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 14.2/25.3 MB 2.1 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 14.2/25.3 MB 2.1 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 14.3/25.3 MB 2.1 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 14.4/25.3 MB 2.1 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 14.4/25.3 MB 2.1 MB/s eta 0:00:06\n",
      "     ---------------------- ----------------- 14.5/25.3 MB 2.1 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 14.6/25.3 MB 2.1 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 14.7/25.3 MB 2.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 14.7/25.3 MB 2.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 14.8/25.3 MB 2.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 14.8/25.3 MB 2.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 14.9/25.3 MB 2.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 14.9/25.3 MB 2.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 14.9/25.3 MB 2.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 15.0/25.3 MB 2.0 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 15.1/25.3 MB 1.9 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 15.1/25.3 MB 1.9 MB/s eta 0:00:06\n",
      "     ----------------------- ---------------- 15.2/25.3 MB 1.9 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 15.2/25.3 MB 1.9 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 15.3/25.3 MB 1.9 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 15.3/25.3 MB 1.9 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 15.4/25.3 MB 1.9 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 15.4/25.3 MB 1.9 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 15.5/25.3 MB 1.9 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 15.6/25.3 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 15.6/25.3 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 15.7/25.3 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 15.7/25.3 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 15.8/25.3 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------------------ --------------- 15.8/25.3 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 15.9/25.3 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 15.9/25.3 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 16.0/25.3 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 16.1/25.3 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 16.2/25.3 MB 1.8 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 16.4/25.3 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 16.6/25.3 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 16.8/25.3 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 16.9/25.3 MB 1.8 MB/s eta 0:00:05\n",
      "     -------------------------- ------------- 17.0/25.3 MB 1.8 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 17.1/25.3 MB 1.8 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 17.3/25.3 MB 1.8 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 17.4/25.3 MB 1.8 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 17.6/25.3 MB 1.8 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 17.7/25.3 MB 1.8 MB/s eta 0:00:05\n",
      "     ---------------------------- ----------- 17.7/25.3 MB 1.9 MB/s eta 0:00:05\n",
      "     ---------------------------- ----------- 17.9/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 18.1/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 18.1/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 18.3/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 18.3/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 18.5/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 18.5/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 18.7/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 18.8/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 18.9/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 19.0/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 19.0/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 19.1/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 19.2/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 19.3/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 19.4/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 19.5/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 19.5/25.3 MB 1.9 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 19.6/25.3 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 19.8/25.3 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 19.9/25.3 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 19.9/25.3 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 20.1/25.3 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 20.3/25.3 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 20.4/25.3 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 20.6/25.3 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 20.7/25.3 MB 1.9 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 20.8/25.3 MB 2.0 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 20.9/25.3 MB 1.9 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 21.0/25.3 MB 2.0 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 21.0/25.3 MB 2.0 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 21.1/25.3 MB 2.0 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 21.2/25.3 MB 2.0 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 21.2/25.3 MB 2.0 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 21.3/25.3 MB 2.0 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 21.3/25.3 MB 2.0 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 21.4/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 21.5/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 21.6/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 21.8/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 21.9/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 22.0/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 22.1/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 22.2/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 22.3/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 22.4/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 22.5/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 22.6/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 22.7/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 22.7/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 22.8/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 22.9/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 23.0/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 23.1/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 23.2/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 23.2/25.3 MB 2.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 23.3/25.3 MB 1.9 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 23.4/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 23.5/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 23.5/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 23.6/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 23.7/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 23.7/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 23.8/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 23.9/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 24.0/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 24.1/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 24.2/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 24.3/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 24.4/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 24.5/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 24.5/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 24.6/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  24.7/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  24.8/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  24.9/25.3 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.0/25.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.1/25.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.1/25.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.2/25.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.3/25.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.3/25.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.3/25.3 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 25.3/25.3 MB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in d:\\software\\anaconda3\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in d:\\software\\anaconda3\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting tqdm\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/52/1c/fa3b61c0cf03e1da4767213672efe186b1dfa4fc901a4a694fb184a513d1/xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/50/15/b56e50e8debaf439f44befec5b2af11db85f6e0f344c3113ae0be0593a91/multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Requirement already satisfied: aiohttp in d:\\software\\anaconda3\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in d:\\software\\anaconda3\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from wandb) (3.1.37)\n",
      "Requirement already satisfied: platformdirs in d:\\software\\anaconda3\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in d:\\software\\anaconda3\\lib\\site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in d:\\software\\anaconda3\\lib\\site-packages (from wandb) (2.9.2)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/12/7f/0e4459173e9671ba5f75a48dda2442bcc48a12c79e54e5789381c8c6a9bc/sentry_sdk-2.22.0-py2.py3-none-any.whl (325 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/e7/57/6e937ac7aa52db69225f02db2cfdcb66ba1db6fdc65a4ddbdf78e214f72a/setproctitle-1.3.5-cp311-cp311-win_amd64.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in d:\\software\\anaconda3\\lib\\site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: colorama in d:\\software\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in d:\\software\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\software\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\software\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\software\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\software\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\software\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\software\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\software\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\software\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\software\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\software\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in d:\\software\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Installing collected packages: xxhash, tqdm, setproctitle, sentry-sdk, safetensors, requests, pyarrow, docker-pycreds, dill, multiprocess, huggingface-hub, wandb, tokenizers, transformers, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.24.7\n",
      "    Uninstalling huggingface-hub-0.24.7:\n",
      "      Successfully uninstalled huggingface-hub-0.24.7\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.0\n",
      "    Uninstalling tokenizers-0.20.0:\n",
      "      Successfully uninstalled tokenizers-0.20.0\n",
      "Successfully installed datasets-3.3.2 dill-0.3.8 docker-pycreds-0.4.0 huggingface-hub-0.29.1 multiprocess-0.70.16 pyarrow-19.0.1 requests-2.32.3 safetensors-0.5.3 sentry-sdk-2.22.0 setproctitle-1.3.5 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.49.0 wandb-0.19.7 xxhash-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.9.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
      "langchain-anthropic 0.1.23 requires langchain-core<0.3.0,>=0.2.26, but you have langchain-core 0.3.12 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# 最好新开conda环境\n",
    "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c60cc4fda5f40a0bfd79c4d017a0a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fanzirong\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559d0954815c42a28931b9a3c463929c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e371c1205d4f65b8fa0832d596df35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M，如果使用1.5B版本换成\"gpt2-xl\"\n",
    "sd_hf = model_hf.state_dict() # 即原始张量raw tensors\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "transformer.wte.weight torch.Size([50257, 768]) # weight of token embedding, bert中有self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "transformer.wpe.weight torch.Size([1024, 768]) # position embedding, GPT-2的max_len=1024，即每个token最多可以关注1024个位置\n",
    "transformer.h.0.ln_1.weight torch.Size([768])\n",
    "transformer.h.0.ln_1.bias torch.Size([768])\n",
    "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.0.ln_2.weight torch.Size([768])\n",
    "transformer.h.0.ln_2.bias torch.Size([768])\n",
    "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.h.1.ln_1.weight torch.Size([768])\n",
    "transformer.h.1.ln_1.bias torch.Size([768])\n",
    "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.1.ln_2.weight torch.Size([768])\n",
    "transformer.h.1.ln_2.bias torch.Size([768])\n",
    "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.h.2.ln_1.weight torch.Size([768])\n",
    "transformer.h.2.ln_1.bias torch.Size([768])\n",
    "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.2.ln_2.weight torch.Size([768])\n",
    "transformer.h.2.ln_2.bias torch.Size([768])\n",
    "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.h.3.ln_1.weight torch.Size([768])\n",
    "transformer.h.3.ln_1.bias torch.Size([768])\n",
    "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.3.ln_2.weight torch.Size([768])\n",
    "transformer.h.3.ln_2.bias torch.Size([768])\n",
    "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.h.4.ln_1.weight torch.Size([768])\n",
    "transformer.h.4.ln_1.bias torch.Size([768])\n",
    "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.4.ln_2.weight torch.Size([768])\n",
    "transformer.h.4.ln_2.bias torch.Size([768])\n",
    "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.h.5.ln_1.weight torch.Size([768])\n",
    "transformer.h.5.ln_1.bias torch.Size([768])\n",
    "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.5.ln_2.weight torch.Size([768])\n",
    "transformer.h.5.ln_2.bias torch.Size([768])\n",
    "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.h.6.ln_1.weight torch.Size([768])\n",
    "transformer.h.6.ln_1.bias torch.Size([768])\n",
    "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.6.ln_2.weight torch.Size([768])\n",
    "transformer.h.6.ln_2.bias torch.Size([768])\n",
    "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.h.7.ln_1.weight torch.Size([768])\n",
    "transformer.h.7.ln_1.bias torch.Size([768])\n",
    "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.7.ln_2.weight torch.Size([768])\n",
    "transformer.h.7.ln_2.bias torch.Size([768])\n",
    "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.h.8.ln_1.weight torch.Size([768])\n",
    "transformer.h.8.ln_1.bias torch.Size([768])\n",
    "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.8.ln_2.weight torch.Size([768])\n",
    "transformer.h.8.ln_2.bias torch.Size([768])\n",
    "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.h.9.ln_1.weight torch.Size([768])\n",
    "transformer.h.9.ln_1.bias torch.Size([768])\n",
    "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.9.ln_2.weight torch.Size([768])\n",
    "transformer.h.9.ln_2.bias torch.Size([768])\n",
    "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.h.10.ln_1.weight torch.Size([768])\n",
    "transformer.h.10.ln_1.bias torch.Size([768])\n",
    "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.10.ln_2.weight torch.Size([768])\n",
    "transformer.h.10.ln_2.bias torch.Size([768])\n",
    "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.h.11.ln_1.weight torch.Size([768])\n",
    "transformer.h.11.ln_1.bias torch.Size([768])\n",
    "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
    "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
    "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
    "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
    "transformer.h.11.ln_2.weight torch.Size([768])\n",
    "transformer.h.11.ln_2.bias torch.Size([768])\n",
    "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
    "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
    "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
    "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
    "transformer.ln_f.weight torch.Size([768])\n",
    "transformer.ln_f.bias torch.Size([768])\n",
    "lm_head.weight torch.Size([50257, 768])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_hf[\"transformer.wpe.weight\"].view(-1)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(sd_hf[\"transformer.wpe.weight\"], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 150])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 200])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sd_hf[\"transformer.h.1.attn.c_attn.weight\"][:300,:300], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's instead sample manually\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "tokens = [15496, 11, 314, 1101, 257, 3303, 2746, 11] # \"Hello, I'm a language model,\"\n",
    "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(5, 1) # (5, 8)\n",
    "x = tokens.to('cuda')\n",
    "\n",
    "# generate!\n",
    "while x.size(1) < 30: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)[0] # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# print the generated text\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "for i in range(5):\n",
    "    tokens = x[i, :30].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "data = text[:1000] # first 1,000 characters\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(data)\n",
    "print(tokens[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "buf = torch.tensor(tokens[:24 + 1])\n",
    "x = buf[:-1].view(4, 6)\n",
    "y = buf[1:].view(4, 6)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sd_hf[\"lm_head.weight\"].shape)\n",
    "print(sd_hf[\"transformer.wte.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sd_hf[\"lm_head.weight\"] == sd_hf[\"transformer.wte.weight\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sd_hf[\"lm_head.weight\"].data_ptr())\n",
    "print(sd_hf[\"transformer.wte.weight\"].data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# standard deviation grows inside the residual stream\n",
    "x = torch.zeros(768)\n",
    "n = 100 # e.g. 100 layers\n",
    "for i in range(n):\n",
    "    x += n**-0.5 * torch.randn(768)\n",
    "\n",
    "print(x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# super simple little MLP\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(16, 32),\n",
    "    torch.nn.GELU(),\n",
    "    torch.nn.Linear(32, 1)\n",
    ")\n",
    "torch.random.manual_seed(42)\n",
    "x = torch.randn(4, 16)\n",
    "y = torch.randn(4, 1)\n",
    "net.zero_grad()\n",
    "yhat = net(x)\n",
    "loss = torch.nn.functional.mse_loss(yhat, y)\n",
    "loss.backward()\n",
    "print(net[0].weight.grad.view(-1)[:10])\n",
    "\n",
    "# the loss objective here is (due to readuction='mean')\n",
    "# L = 1/4 * [\n",
    "#            (y[0] - yhat[0])**2 +\n",
    "#            (y[1] - yhat[1])**2 +\n",
    "#            (y[2] - yhat[2])**2 +\n",
    "#            (y[3] - yhat[3])**2\n",
    "#           ]\n",
    "# NOTE: 1/4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's do it with grad_accum_steps of 4, and B=1\n",
    "# the loss objective here is different because\n",
    "# accumulation in gradient <---> SUM in loss\n",
    "# i.e. we instead get:\n",
    "# L0 = 1/4(y[0] - yhat[0])**2\n",
    "# L1 = 1/4(y[1] - yhat[1])**2\n",
    "# L2 = 1/4(y[2] - yhat[2])**2\n",
    "# L3 = 1/4(y[3] - yhat[3])**2\n",
    "# L = L0 + L1 + L2 + L3\n",
    "# NOTE: the \"normalizer\" of 1/4 is lost\n",
    "net.zero_grad()\n",
    "for i in range(4):\n",
    "    yhat = net(x[i])\n",
    "    loss = torch.nn.functional.mse_loss(yhat, y[i])\n",
    "    loss = loss / 4 # <-- have to add back the \"normalizer\"!\n",
    "    loss.backward()\n",
    "print(net[0].weight.grad.view(-1)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse and visualize the logfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sz = \"124M\"\n",
    "\n",
    "loss_baseline = {\n",
    "    \"124M\": 3.2924,\n",
    "}[sz]\n",
    "hella2_baseline = { # HellaSwag for GPT-2\n",
    "    \"124M\": 0.294463,\n",
    "    \"350M\": 0.375224,\n",
    "    \"774M\": 0.431986,\n",
    "    \"1558M\": 0.488946,\n",
    "}[sz]\n",
    "hella3_baseline = { # HellaSwag for GPT-3\n",
    "    \"124M\": 0.337,\n",
    "    \"350M\": 0.436,\n",
    "    \"774M\": 0.510,\n",
    "    \"1558M\": 0.547,\n",
    "}[sz]\n",
    "\n",
    "# load the log file\n",
    "with open(\"log124M_40B/log.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# parse the individual lines, group by stream (train,val,hella)\n",
    "streams = {}\n",
    "for line in lines:\n",
    "    step, stream, val = line.strip().split()\n",
    "    if stream not in streams:\n",
    "        streams[stream] = {}\n",
    "    streams[stream][int(step)] = float(val)\n",
    "\n",
    "# convert each stream from {step: val} to (steps[], vals[])\n",
    "# so it's easier for plotting\n",
    "streams_xy = {}\n",
    "for k, v in streams.items():\n",
    "    # get all (step, val) items, sort them\n",
    "    xy = sorted(list(v.items()))\n",
    "    # unpack the list of tuples to tuple of lists\n",
    "    streams_xy[k] = list(zip(*xy))\n",
    "\n",
    "# create figure\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Panel 1: losses: both train and val\n",
    "plt.subplot(121)\n",
    "xs, ys = streams_xy[\"train\"] # training loss\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f'nanogpt ({sz}) train loss')\n",
    "print(\"Min Train Loss:\", min(ys))\n",
    "xs, ys = streams_xy[\"val\"] # validation loss\n",
    "plt.plot(xs, ys, label=f'nanogpt ({sz}) val loss')\n",
    "# horizontal line at GPT-2 baseline\n",
    "if loss_baseline is not None:\n",
    "    plt.axhline(y=loss_baseline, color='r', linestyle='--', label=f\"OpenAI GPT-2 ({sz}) checkpoint val loss\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale('log')\n",
    "plt.ylim(top=4.0)\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "print(\"Min Validation Loss:\", min(ys))\n",
    "\n",
    "# Panel 2: HellaSwag eval\n",
    "plt.subplot(122)\n",
    "xs, ys = streams_xy[\"hella\"] # HellaSwag eval\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f\"nanogpt ({sz})\")\n",
    "# horizontal line at GPT-2 baseline\n",
    "if hella2_baseline:\n",
    "    plt.axhline(y=hella2_baseline, color='r', linestyle='--', label=f\"OpenAI GPT-2 ({sz}) checkpoint\")\n",
    "if hella3_baseline:\n",
    "    plt.axhline(y=hella3_baseline, color='g', linestyle='--', label=f\"OpenAI GPT-3 ({sz}) checkpoint\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"HellaSwag eval\")\n",
    "print(\"Max Hellaswag eval:\", max(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
